<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations">
  <meta name="keywords" content="OpenM3D, open vocabulary 3D detection, multi-view 3D, label-free 3D, indoor 3D object detection, open-vocabulary">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</title>

  <!-- Canonical URL -->
  <link rel="canonical" href="https://penghaohsu.github.io/projects/openm3d/">

  <!-- (Optional) keywords: not used by Google much, but harmless and helpful for some engines -->
  <meta name="keywords" content="OpenM3D, open vocabulary 3D detection, multi-view 3D, label-free 3D, indoor 3D object detection, open-vocabulary, voxel-semantic alignment">

  <!-- Open Graph -->
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://penghaohsu.github.io/projects/openm3d/">
  <meta property="og:title" content="OpenM3D: Open-Vocabulary Multi-view Indoor 3D Object Detection">
  <meta property="og:description" content="OpenM3D is a first multi-view open-vocabulary 3D detector trained without human annotations. RGB-only inference (no depth) and 0.3 s/scene.">
  <meta property="og:image" content="https://penghaohsu.github.io/static/images/openm3d_qualitative.jpg">

  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <!-- Root-relative asset paths so the page works at /projects/openm3d/ -->
  <link rel="stylesheet" href="/static/css/bulma.min.css">
  <link rel="stylesheet" href="/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="/static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="/static/js/fontawesome.all.min.js"></script>
  <script src="/static/js/bulma-carousel.min.js"></script>
  <script src="/static/js/bulma-slider.min.js"></script>
  <script src="/static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://penghaohsu.github.io/">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</h1>
          <h2 class="title is-size-3 publication-title">ICCV'25</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://penghaohsu.github.io/">Peng-Hao Hsu</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Ke Zhang</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://fuenwang.phd/">Fu-En Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://ttaoretw.github.io/">Tao Tu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Ming-Feng Li</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://yulunalexliu.github.io/">Yu-Lun Liu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Albert Y. C. Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://aliensunmin.github.io/">Min Sun</a><sup>1, 2†</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="">Cheng-Hao Kuo</a><sup>2†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Tsing Hua University</span>&nbsp;&nbsp;,
            <span class="author-block"><sup>2</sup>Amazon</span>,
            <span class="author-block"><sup>3</sup>Cornell University</span>,
            <span class="author-block"><sup>4</sup>Carnegie Mellon University</span>,
            <span class="author-block"><sup>5</sup>National Yang Ming Chiao Tung University</span>
          </div>
          
          <div class="column has-text-centered">
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><b>ICCV, 2025</b></span>
            </div> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2508.20063"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video (moved above Abstract; heading removed) -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/aHqqYG7outg?si=lfU6tKbXMY4t-7HM"
              frameborder="0" allow="autoplay controls; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
          <p>
            Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available.
          </p>
          <p>
            We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.
          </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    
    <h2 class="title is-3">Motivation</h2>
    <div class="content has-text-justified mb-6" style="line-height:1.7">
      <p>
        Open-vocabulary (OV) 3D object detection for indoor scenes has mostly relied on point-cloud inputs, 
        which require costly 3D sensors and limit deployment. In contrast, multi-view image–based methods have 
        recently achieved strong <em>fixed-vocabulary</em> 3D detection but do not support open vocabularies. 
        <strong>OpenM3D</strong> closes this gap: it is the first multi-view, single-stage OV 3D detector trained 
        <em>without human annotations</em>. Built on geometry-shaped voxel features from ImGeoNet, OpenM3D learns 
        to localize objects with class-agnostic supervision from high-quality <em>3D pseudo boxes</em> and to recognize 
        open-set categories via a novel <em>Voxel–Semantic Alignment</em> that matches 3D voxel features with diverse 
        CLIP embeddings sampled from multi-view segments.
      </p>
      <p>
        During inference, OpenM3D needs only multi-view RGB images and camera poses—no depth maps and no CLIP 
        computation—making it highly efficient (<em>~0.3 s per scene</em>) while outperforming strong baselines such as 
        OV-3DET and OpenMask3D on ScanNet200 and ARKitScenes in accuracy and speed. This brings practical, scalable 
        OV 3D perception to applications like robotics and AR, without the overhead of expensive sensors or manual labels.
      </p>
    </div>

    <h2 class="title is-3" style="margin-top: 2.25rem;">Method</h2>
    <div class="content has-text-justified mb-4" style="line-height:1.7">
      <p>
        <strong>OpenM3D</strong> is a <em>single-stage</em> open-vocabulary (OV) multi-view 3D detector trained
        <strong>without human annotations</strong>. It adapts ImGeoNet's geometry-shaped <em>voxel features</em> and learns from two
        complementary signals: a <em>class-agnostic 3D localization</em> objective supervised by high-quality <em>3D pseudo boxes</em>,
        and an <em>open-set recognition</em> objective by aligning 3D voxel features with diverse CLIP embeddings sampled from
        multi-view segments. At inference, only multi-view RGB images and camera poses are needed—no depth or CLIP forward pass—
        enabling efficient (~0.3 s/scene) and accurate OV 3D detection.
      </p>
    </div>

    <!-- 3D Pseudo Box Generation figure (export from paper and save as /static/images/openm3d_pseudo_boxes.png) -->
    <div class="columns is-centered has-text-centered my-5">
      <div class="column is-four-fifths">
        <figure>
          <img src="/static/images/pseudo_box.jpg" alt="3D Pseudo Box Generation" />
          <figcaption class="has-text-grey is-size-6">
            <em>3D Pseudo Box Generation.</em> Graph-embedding association groups multi-view 2D segments into coherent 3D structures
            for fitting oriented pseudo boxes.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="content has-text-justified">
      <p>
        <strong>3D Pseudo Box Generation.</strong> From posed multi-view images, we obtain 2D segments and build a cross-view
        association graph using geometric consistency (epipolar constraints, depth checks) and appearance cues. Coherent
        multi-view groups are merged into 3D structures, to which we fit oriented bounding boxes as <em>pseudo boxes</em>. These
        serve as high-quality training targets for a class-agnostic localization head and achieve higher precision/recall than
        prior methods.
      </p>
    </div>

    <!-- Pipeline / Overview figure (export from paper and save as /static/images/openm3d_method_overview.png) -->
    <div class="columns is-centered has-text-centered my-6">
      <div class="column is-four-fifths">
        <figure>
          <img src="/static/images/openm3d_model.jpg" alt="OpenM3D Training & Inference Pipeline" />
          <figcaption class="has-text-grey is-size-6">
            <em>Pipeline.</em> Training leverages pseudo boxes for class-agnostic localization and diverse CLIP features for
            voxel–semantic alignment on ImGeoNet voxel features. Inference uses only multi-view RGB + poses.
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="content has-text-justified" style="line-height:1.7">
      <p>
        <strong>Pipeline.</strong> During <em>training</em>, ImGeoNet encodes multi-view images into voxel features. The detector head is
        supervised by (i) class-agnostic 3D localization to the generated pseudo boxes, and (ii) voxel–semantic alignment that
        matches voxel features to CLIP embeddings sampled from the associated multi-view segments—enabling open-vocabulary
        recognition in a <em>single stage</em>. During <em>inference</em>, we predict 3D boxes and OV scores directly from multi-view RGB
        and camera poses without depth or CLIP computation, providing both accuracy and speed.
      </p>
    </div>


  </div>
</section>


<section class="section" id="Results">
    <!-- Result. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Quantitative Results</h2>
        <h3 class="title is-5" style="margin-top:1rem;">3D Pseudo Box Evaluation on ScanNet200 and ARKitScenes</h3>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="table_wrapper">
              <table class="table is-hoverable is-bordered is-fullwidth">
                <thead>
                  <tr>
                    <th rowspan="2" style="vertical-align:middle;text-align:left;">Method</th>
                    <th colspan="4" style="text-align:center;">ScanNet200</th>
                    <th colspan="4" style="text-align:center;">ARKitScenes</th>
                  </tr>
                  <tr>
                    <th style="text-align:center;">Prec@0.25</th>
                    <th style="text-align:center;">Prec@0.5</th>
                    <th style="text-align:center;">Rec@0.25</th>
                    <th style="text-align:center;">Rec@0.5</th>
                    <th style="text-align:center;">Prec@0.25</th>
                    <th style="text-align:center;">Prec@0.5</th>
                    <th style="text-align:center;">Rec@0.25</th>
                    <th style="text-align:center;">Rec@0.5</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align:left;">Ours (w/ MSR)</td>
                    <td style="text-align:center;">32.07</td>
                    <td style="text-align:center;">18.14</td>
                    <td style="text-align:center;">58.30</td>
                    <td style="text-align:center;">32.99</td>
                    <td style="text-align:center;">5.97</td>
                    <td style="text-align:center;">1.58</td>
                    <td style="text-align:center;">51.92</td>
                    <td style="text-align:center;">13.74</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">Ours (w/o MSR)</td>
                    <td style="text-align:center;">27.09</td>
                    <td style="text-align:center;">11.98</td>
                    <td style="text-align:center;">52.43</td>
                    <td style="text-align:center;">23.18</td>
                    <td style="text-align:center;">6.06</td>
                    <td style="text-align:center;">1.34</td>
                    <td style="text-align:center;">51.40</td>
                    <td style="text-align:center;">11.41</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OV-3DET [28]</td>
                    <td style="text-align:center;">11.62</td>
                    <td style="text-align:center;">4.40</td>
                    <td style="text-align:center;">21.13</td>
                    <td style="text-align:center;">7.99</td>
                    <td style="text-align:center;">3.74</td>
                    <td style="text-align:center;">0.91</td>
                    <td style="text-align:center;">32.43</td>
                    <td style="text-align:center;">7.93</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">SAM3D [55]</td>
                    <td style="text-align:center;">14.48</td>
                    <td style="text-align:center;">9.05</td>
                    <td style="text-align:center;">57.70</td>
                    <td style="text-align:center;">36.07</td>
                    <td style="text-align:center;">6.01</td>
                    <td style="text-align:center;">1.49</td>
                    <td style="text-align:center;">43.78</td>
                    <td style="text-align:center;">10.87</td>
                  </tr>
                </tbody>
              </table>
              <p class="is-size-7 has-text-grey" style="margin-top:0.5rem;">
                <em>Note.</em> Our boxes, with and without Mesh Segmentation Refinement (MSR), exceed OV-3DET and SAM3D in precision at IoU 0.25/0.5 across both datasets. (a) See supplementary for head/common/tail breakdown on ScanNet200. (b) <span class="has-text-grey">* precision on ARKitScenes is expected to be low since only 17 classes are labeled; boxes on unlabeled objects are counted as false positives.</span>
              </p>
            </div>
          </div>
        </div>

        <h3 class="title is-5" style="margin-top:1.5rem;">Class-agnostic 3D Object Detection on ScanNet200</h3>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="table_wrapper">
              <table class="table is-hoverable is-bordered is-fullwidth">
                <thead>
                  <tr>
                    <th style="text-align:left;">Method</th>
                    <th style="text-align:left;">Trained Box / Candidate Box</th>
                    <th style="text-align:center;">mAP@0.25</th>
                    <th style="text-align:center;">mAR@0.25</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">OV-3DET [28]</td>
                    <td style="text-align:center;">3.13</td>
                    <td style="text-align:center;">10.83</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">SAM3D [55]</td>
                    <td style="text-align:center;">3.92</td>
                    <td style="text-align:center;">13.33</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">Ours (w/o MSR)</td>
                    <td style="text-align:center;">4.04</td>
                    <td style="text-align:center;">13.77</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">Ours</td>
                    <td style="text-align:center;">4.23</td>
                    <td style="text-align:center;">15.12</td>
                  </tr>
                </tbody>
              </table>
              <p class="is-size-7 has-text-grey" style="margin-top:0.5rem;">
                <em>Note.</em> Our single-stage framework benefits from higher-quality pseudo boxes and achieves stronger class-agnostic detection on ScanNet200.
              </p>
            </div>
          </div>
        </div>

        <h3 class="title is-5" style="margin-top:1.5rem;">3D Object Detection on ScanNet200</h3>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="table_wrapper">
              <table class="table is-hoverable is-bordered is-fullwidth">
                <thead>
                  <tr>
                    <th style="text-align:left;">Method</th>
                    <th style="text-align:left;">Trained Box</th>
                    <th style="text-align:center;">mAP@0.25</th>
                    <th style="text-align:center;">mAR@0.25</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">OV-3DET [28]</td>
                    <td style="text-align:center;">19.53</td>
                    <td style="text-align:center;">35.19</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">SAM3D [55]</td>
                    <td style="text-align:center;">23.77</td>
                    <td style="text-align:center;">47.82</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">Ours (w/o MSR)</td>
                    <td style="text-align:center;">25.95</td>
                    <td style="text-align:center;">48.14</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:left;">Ours</td>
                    <td style="text-align:center;">26.92</td>
                    <td style="text-align:center;">51.19</td>
                  </tr>
                </tbody>
              </table>
              <p class="is-size-7 has-text-grey" style="margin-top:0.5rem;">
                <em>Note.</em> OpenM3D surpasses two-stage and OV-3DET-trained baselines on ScanNet200, with larger gains on challenging categories.
              </p>
            </div>
          </div>
        </div>

        <h3 class="title is-5" style="margin-top:1.5rem;">3D Object Detection on ScanNetv2</h3>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="table_wrapper">
              <table class="table is-hoverable is-bordered is-fullwidth">
                <thead>
                  <tr>
                    <th style="text-align:left;">Method</th>
                    <th style="text-align:left;">Training Data</th>
                    <th style="text-align:center;">Input</th>
                    <th style="text-align:center;">Detector</th>
                    <th style="text-align:center;">AP@25 (%)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align:left;">OV-3DET† [28]</td>
                    <td style="text-align:left;">ScanNet</td>
                    <td style="text-align:center;">pc + im</td>
                    <td style="text-align:center;">Two-Stage</td>
                    <td style="text-align:center;">18.02</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">CoDA† [3]</td>
                    <td style="text-align:left;">ScanNet</td>
                    <td style="text-align:center;">pc</td>
                    <td style="text-align:center;">One-Stage</td>
                    <td style="text-align:center;">19.32</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">ImOV3D† [54]</td>
                    <td style="text-align:left;">ScanNet, LVIS</td>
                    <td style="text-align:center;">pc</td>
                    <td style="text-align:center;">One-Stage</td>
                    <td style="text-align:center;">21.45</td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">Ours</td>
                    <td style="text-align:left;">ScanNet</td>
                    <td style="text-align:center;">im</td>
                    <td style="text-align:center;">One-Stage</td>
                    <td style="text-align:center;">19.76</td>
                  </tr>
                </tbody>
              </table>
              <p class="is-size-7 has-text-grey" style="margin-top:0.5rem;">
                <em>Notes.</em> † denotes OV-adapted variants from prior work. "pc" = point cloud, "im" = RGB images. Numbers are AP@25 (%) on ScanNetv2. ImOV3D additionally uses LVIS during training.
              </p>
              <div class="content has-text-justified" style="line-height:1.7">
                <p>
                  Despite using only image inputs (<em>im</em>) and a single-stage head, <strong>OpenM3D</strong> achieves <strong>19.76 AP@25</strong> on ScanNetv2, outperforming the two-stage OV-3DET (pc+im) and approaching point-cloud–based one-stage methods. This highlights the competitiveness of our RGB-only, single-stage OV 3D detector under fair training data.
                </p>
              </div>
            </div>
          </div>
        </div>

        <h3 class="title is-5" style="margin-top:1.5rem;">3D Object Detection on ARKitScenes</h3>
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="table_wrapper">
              <table class="table is-hoverable is-bordered is-fullwidth">
                <thead>
                  <tr>
                    <th style="text-align:left;">Method</th>
                    <th style="text-align:center;">Trained Box</th>
                    <th style="text-align:center;">mAR@25 (%)</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td style="text-align:left;">OpenM3D</td>
                    <td style="text-align:center;">Ours</td>
                    <td style="text-align:center;"><strong>42.77</strong></td>
                  </tr>
                  <tr>
                    <td style="text-align:left;">S2D</td>
                    <td style="text-align:center;">Ours</td>
                    <td style="text-align:center;">19.58</td>
                  </tr>
                </tbody>
              </table>
              <div class="columns">
                <div class="column is-offset-7 is-5">
                  <div class="content is-size-7 has-text-left" style="font-style:italic;">
                    <p><strong>Note:</strong><br>
                    We do not report mAP on ARKitScenes due to limited classes (only 17), where detections on unlabeled objects are counted as false positives.</p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{hsu2025openm3d,
  title={OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations},
  author={Hsu, Peng-Hao and Zhang, Ke and Wang, Fu-En and Tu, Tao and Li, Ming-Feng and Liu, Yu-Lun and Chen, Albert YC and Sun, Min and Kuo, Cheng-Hao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2025}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/penghaohsu" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content has-text-centered">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>